# 东方财富爬虫 - Apify Actor 升级说明

## 升级概述

本次升级将原有的东方财富股吧爬虫项目从 Selenium + MongoDB 方案升级为 Apify Actor 版本，实现了云端部署和标准化数据输出。

## 版本演进

### v1.0 - Selenium 版本
- 使用 Selenium WebDriver 进行爬取
- 数据存储到本地 MongoDB
- 需要手动管理 ChromeDriver
- 支持多线程爬取

### v2.0 - Selenium 优化版
- 添加反检测机制（stealth.min.js）
- 优化帖子和评论解析逻辑
- 改进错误处理和断点续爬

### v3.0 - Playwright 版本
- 迁移到 Playwright 框架
- 原生异步支持，性能提升
- 无需手动管理浏览器驱动
- 数据输出为 JSON 文件

### v4.0 - Apify Actor 版本（当前）
- 集成 Apify SDK
- 支持云端部署和本地运行
- 标准化输入输出
- Docker 容器化
- 支持调度任务和监控

## 核心改动

### 1. 项目结构变化

#### 新增文件
```
.actor/
├── actor.json              # Actor 元数据配置
└── INPUT_SCHEMA.json       # 输入参数 JSON Schema 定义

Dockerfile                  # Docker 构建配置
.dockerignore              # Docker 构建忽略文件
.gitignore                 # Git 忽略文件
APIFY_DEPLOYMENT.md        # Apify 部署指南
项目升级说明.md             # 本文档
```

#### 修改文件
```
main.py                    # 重写为 Apify Actor 入口
requirements.txt           # 更新依赖为 apify + playwright
README.md                  # 更新文档说明
```

#### 保留文件
```
main_old.py                # 原 Selenium 版本主程序（重命名）
crawler.py                 # Selenium 爬虫逻辑
parser.py                  # 数据解析器
mongodb.py                 # MongoDB 接口
playwright_eastmoney_crawler.py  # 独立 Playwright 版本
```

### 2. 技术栈变化

| 组件 | v2.0 (Selenium) | v4.0 (Apify Actor) |
|------|-----------------|---------------------|
| 浏览器自动化 | Selenium | Playwright |
| 数据存储 | MongoDB | Apify Dataset |
| 运行环境 | 本地 Python | Docker 容器 |
| 部署方式 | 手动部署 | Apify 平台 |
| 输入方式 | 代码硬编码 | JSON 配置 |
| 输出格式 | MongoDB | JSON/CSV/Excel |

### 3. 代码架构变化

#### v2.0 架构（Selenium）
```python
# main.py
from crawler import PostCrawler, CommentCrawler
import threading

def post_thread(stock_symbol, start_page, end_page):
    post_crawler = PostCrawler(stock_symbol)
    post_crawler.crawl_post_info(start_page, end_page)

thread1 = threading.Thread(target=post_thread, args=('000333', 1, 500))
thread1.start()
```

#### v4.0 架构（Apify Actor）
```python
# main.py
from apify import Actor
from playwright.async_api import async_playwright

async def main() -> None:
    async with Actor:
        # 获取输入
        actor_input = await Actor.get_input() or {}
        stock_code = actor_input.get('stockCode', '002001')

        # 爬取数据
        async with async_playwright() as playwright:
            browser = await playwright.chromium.launch(...)
            # ... 爬取逻辑

        # 推送数据
        await Actor.push_data(posts_data)
```

### 4. 配置文件详解

#### .actor/actor.json
```json
{
  "actorSpecification": 1,
  "name": "eastmoney-stock-forum-crawler",
  "title": "EastMoney Stock Forum Crawler",
  "description": "A web scraper that crawls stock forum posts...",
  "version": "1.0",
  "dockerfile": "./Dockerfile",
  "input": "./INPUT_SCHEMA.json",
  "minMemoryMbytes": 512,
  "maxMemoryMbytes": 2048
}
```

**说明：**
- `actorSpecification`: 必须为 1
- `name`: Actor 唯一标识符
- `dockerfile`: Docker 构建文件路径
- `input`: 输入参数定义文件
- `minMemoryMbytes`: 最小内存需求

#### .actor/INPUT_SCHEMA.json
```json
{
  "title": "EastMoney Stock Forum Crawler Input",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
    "stockCode": {
      "title": "Stock Code",
      "type": "string",
      "description": "The stock code to crawl",
      "default": "002001",
      "editor": "textfield"
    }
  },
  "required": ["stockCode", "stockName"]
}
```

**说明：**
- 基于 JSON Schema 标准
- Apify 自动生成输入表单
- 支持类型验证和默认值
- `editor` 指定输入控件类型

#### Dockerfile
```dockerfile
FROM apify/actor-python:3.11

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# 安装 Playwright 浏览器
RUN pip install playwright apify && \
    playwright install chromium && \
    playwright install-deps chromium

COPY . ./
CMD python3 -u main.py
```

**说明：**
- 基于 Apify 官方 Python 镜像
- 自动安装 Playwright 和 Chromium
- 包含所有系统依赖

### 5. 数据流变化

#### v2.0 数据流
```
输入（代码参数）
    ↓
Selenium 爬取
    ↓
数据解析（parser.py）
    ↓
MongoDB 存储
    ↓
手动导出查询
```

#### v4.0 数据流
```
输入（JSON 配置）
    ↓
Actor.get_input()
    ↓
Playwright 爬取
    ↓
数据处理
    ↓
Actor.push_data()
    ↓
Apify Dataset（自动支持多种导出格式）
```

## 功能对比

| 功能 | v2.0 | v4.0 | 说明 |
|------|------|------|------|
| 爬取帖子 | ✅ | ✅ | 两版本都支持 |
| 爬取评论 | ✅ | ✅ | v4.0 可选开关 |
| 多线程 | ✅ | ❌ | v4.0 使用异步 |
| 断点续爬 | ✅ | ❌ | v4.0 简化逻辑 |
| MongoDB 存储 | ✅ | ❌ | v4.0 使用 Dataset |
| JSON 导出 | ❌ | ✅ | v4.0 原生支持 |
| CSV 导出 | ❌ | ✅ | v4.0 一键导出 |
| 云端运行 | ❌ | ✅ | v4.0 核心特性 |
| 调度任务 | ❌ | ✅ | v4.0 平台功能 |
| API 访问 | ❌ | ✅ | v4.0 平台功能 |
| Webhook | ❌ | ✅ | v4.0 平台功能 |

## 使用场景对比

### 适合使用 v2.0 (Selenium) 的场景
- 需要爬取大量历史数据（数百万条）
- 需要断点续爬功能
- 已有 MongoDB 基础设施
- 需要多线程并发爬取
- 本地环境运行，无需云端部署

### 适合使用 v4.0 (Apify Actor) 的场景
- 需要定期自动爬取最新数据
- 需要云端运行，无本地环境
- 需要标准化数据输出（JSON/CSV）
- 需要 API 访问爬取结果
- 需要监控和告警功能
- 需要与其他系统集成

### 适合使用 v3.0 (独立 Playwright) 的场景
- 快速本地测试
- 不需要 Apify 平台
- 不需要数据库存储
- 简单的一次性爬取任务

## 迁移指南

### 从 v2.0 迁移到 v4.0

1. **环境准备**
   ```bash
   # 不再需要 MongoDB
   # 不再需要 ChromeDriver

   # 只需要 Python 3.11+
   pip install apify playwright
   playwright install chromium
   ```

2. **配置转换**

   v2.0 配置：
   ```python
   thread1 = threading.Thread(
       target=post_thread,
       args=('000333', 1, 500)
   )
   ```

   v4.0 配置：
   ```json
   {
     "stockCode": "000333",
     "stockName": "股票名称",
     "maxPosts": 500
   }
   ```

3. **数据导出**

   v2.0：从 MongoDB 导出
   ```bash
   mongoexport --db post_info --collection post_000333 --out posts.json
   ```

   v4.0：从 Apify Dataset 下载
   ```bash
   # 在 Apify 控制台点击 "Download" 选择格式
   # 或使用 API
   curl "https://api.apify.com/v2/.../dataset/items" > posts.json
   ```

## 性能对比

### 爬取速度

| 场景 | v2.0 | v4.0 | 说明 |
|------|------|------|------|
| 10 个帖子 | ~15秒 | ~12秒 | v4.0 异步优势 |
| 100 个帖子 | ~2分钟 | ~1.5分钟 | 差距不大 |
| 1000 个帖子 | ~20分钟 | ~18分钟 | v2.0 可多线程 |

### 资源消耗

| 资源 | v2.0 | v4.0 |
|------|------|------|
| 内存 | ~500MB | ~800MB |
| CPU | 中等 | 中等 |
| 磁盘 | MongoDB 空间 | 临时缓存 |
| 网络 | 直连 | 可用代理 |

## 成本分析

### v2.0 成本
- 服务器/VPS：$5-20/月
- MongoDB 存储：$0-10/月
- 开发维护：较高

### v4.0 成本
- Apify 免费套餐：$0/月（5美元额度）
- Apify 付费套餐：按使用量计费
- 开发维护：较低

### 成本示例
假设每天爬取 100 个帖子：
- v2.0: 服务器 $10 + 维护成本
- v4.0: 约 $0.5-2/月（在免费额度内）

## 最佳实践

### 1. 开发建议
- 本地开发使用 v3.0 (独立 Playwright)
- 测试通过后迁移到 v4.0
- 使用 `apify run` 本地测试 Actor

### 2. 生产部署
- 使用 Apify 平台托管
- 配置合理的调度频率
- 启用运行监控和告警
- 定期检查数据质量

### 3. 数据管理
- 定期导出数据备份
- 使用 API 集成到数据管道
- 设置数据保留策略

## 常见问题

### Q1: 旧版本还能用吗？
A: 可以！所有旧版本代码都保留在项目中：
- `main_old.py` - Selenium 版本
- `crawler.py`, `parser.py`, `mongodb.py` - Selenium 支持文件
- `playwright_eastmoney_crawler.py` - 独立 Playwright 版本

### Q2: 如何选择版本？
A:
- 需要云端部署 → v4.0
- 需要大量历史数据 → v2.0
- 快速本地测试 → v3.0

### Q3: v4.0 能爬取更多数据吗？
A: 单次爬取建议不超过 100 个帖子。如需大量数据：
- 使用调度任务分批爬取
- 或使用 v2.0 多线程版本

### Q4: 数据格式有变化吗？
A: 基本一致，只是存储方式不同：
- v2.0: MongoDB 文档
- v4.0: JSON 数组

### Q5: 如何从 v2.0 升级到 v4.0？
A:
1. 保留 v2.0 环境不动
2. 部署 v4.0 到 Apify
3. 并行运行一段时间验证
4. 确认无误后逐步切换

## 后续规划

### 短期目标（1-3个月）
- [ ] 添加更多股票论坛支持
- [ ] 优化数据解析准确度
- [ ] 添加数据清洗功能
- [ ] 提供数据分析示例

### 中期目标（3-6个月）
- [ ] 支持实时数据流
- [ ] 添加情感分析功能
- [ ] 集成数据可视化
- [ ] 提供 REST API

### 长期目标（6-12个月）
- [ ] 多数据源整合
- [ ] 机器学习预测模型
- [ ] 移动端应用
- [ ] 商业化方案

## 贡献和反馈

欢迎提交 Issue 和 Pull Request！

- GitHub: https://github.com/xujfcn/EastMoney_Crawler
- Issues: https://github.com/xujfcn/EastMoney_Crawler/issues

## 参考资源

- [Apify 文档](https://docs.apify.com/)
- [Playwright 文档](https://playwright.dev/python/)
- [项目 README](README.md)
- [部署指南](APIFY_DEPLOYMENT.md)

---

**版本历史**
- 2025-01: v4.0 Apify Actor 版本发布
- 2024-09: v3.0 Playwright 独立版本
- 2023-12: v2.0 Selenium 优化版本
- 2023-01: v1.0 初始版本
