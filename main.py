#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™« - Apify Actor ç‰ˆæœ¬
ä½¿ç”¨ Playwright è¿›è¡Œæ•°æ®çˆ¬å–
"""

import asyncio
import logging
import re
from datetime import datetime
from urllib.parse import urljoin

from playwright.async_api import async_playwright
from apify import Actor


class EastMoneyCrawler:
    """ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™«"""

    def __init__(self, stock_code, stock_name, max_posts=10, headless=True):
        self.stock_code = stock_code
        self.stock_name = stock_name
        self.max_posts = max_posts
        self.headless = headless
        self.base_url = f"https://guba.eastmoney.com/list,{stock_code}.html"
        self.logger = logging.getLogger(__name__)

        # å­˜å‚¨æ•°æ®
        self.posts_data = []
        self.comments_data = []

    async def crawl_post_list(self, page):
        """çˆ¬å–å¸–å­åˆ—è¡¨"""
        try:
            self.logger.info(f"å¼€å§‹çˆ¬å–è‚¡ç¥¨ {self.stock_name}({self.stock_code}) çš„å¸–å­...")

            # è®¿é—®é¡µé¢
            await page.goto(self.base_url, wait_until='domcontentloaded', timeout=45000)
            await asyncio.sleep(3)

            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            title = await page.title()
            if title and ("ä¸œæ–¹è´¢å¯Œ" in title or "è‚¡å§" in title):
                self.logger.info(f"âœ… é¡µé¢åŠ è½½æˆåŠŸ: {title}")
            else:
                self.logger.warning(f"é¡µé¢æ ‡é¢˜å¼‚å¸¸: {title}")
                return 0

            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await asyncio.sleep(3)

            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°å¸–å­åˆ—è¡¨
            selectors = [
                'tr.listitem',
                'tr[class*="listitem"]',
                '.articleh',
                '.normal_post',
                'tbody tr'
            ]

            post_elements = []
            for selector in selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        post_elements = elements
                        break
                except Exception as e:
                    self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue

            if not post_elements:
                self.logger.warning("æœªæ‰¾åˆ°å¸–å­å…ƒç´ ï¼Œå°è¯•è·å–é¡µé¢å†…å®¹è¿›è¡Œåˆ†æ")

                # è·å–é¡µé¢HTMLè¿›è¡Œåˆ†æ
                content = await page.content()

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¸–å­ä¿¡æ¯
                title_pattern = r'<a[^>]*title="([^"]*)"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
                matches = re.findall(title_pattern, content)

                self.logger.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(matches)} ä¸ªæ ‡é¢˜é“¾æ¥")

                count = 0
                for title, href, text in matches[:self.max_posts]:
                    if self.stock_code in href or "guba.eastmoney.com" in href:
                        post_url = href if href.startswith('http') else f"https://guba.eastmoney.com{href}"

                        post_data = {
                            'title': title or text,
                            'author': f'ç”¨æˆ·{count+1}',
                            'post_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'read_count': '0',
                            'reply_count': '0',
                            'post_url': post_url,
                            'stock_code': self.stock_code,
                            'stock_name': self.stock_name,
                            'crawl_time': datetime.now().isoformat()
                        }

                        self.posts_data.append(post_data)
                        count += 1
                        self.logger.info(f"å¸–å­ {count}: {title[:50]}...")

                return len(self.posts_data)

            # å¤„ç†æ‰¾åˆ°çš„å¸–å­å…ƒç´ 
            for i, element in enumerate(post_elements[:self.max_posts]):
                try:
                    # å°è¯•æå–æ ‡é¢˜
                    title_element = await element.query_selector('a[title]')
                    if not title_element:
                        title_element = await element.query_selector('a')

                    if title_element:
                        title = await title_element.get_attribute('title')
                        if not title:
                            title = await title_element.inner_text()

                        href = await title_element.get_attribute('href')
                        post_url = href if href and href.startswith('http') else f"https://guba.eastmoney.com{href}" if href else ""
                    else:
                        title = f"å¸–å­{i+1}"
                        post_url = ""

                    # å°è¯•æå–å…¶ä»–ä¿¡æ¯
                    author = "æœªçŸ¥ç”¨æˆ·"
                    try:
                        author_element = await element.query_selector('.author, .username, [class*="author"]')
                        if author_element:
                            author = await author_element.inner_text()
                    except:
                        pass

                    post_data = {
                        'title': title.strip() if title else f"å¸–å­{i+1}",
                        'author': author.strip() if author else f"ç”¨æˆ·{i+1}",
                        'post_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'read_count': '0',
                        'reply_count': '0',
                        'post_url': post_url,
                        'stock_code': self.stock_code,
                        'stock_name': self.stock_name,
                        'crawl_time': datetime.now().isoformat()
                    }

                    self.posts_data.append(post_data)
                    self.logger.info(f"å¸–å­ {i+1}: {post_data['title'][:50]}...")

                except Exception as e:
                    self.logger.error(f"å¤„ç†å¸–å­ {i+1} æ—¶å‡ºé”™: {e}")
                    continue

            return len(self.posts_data)

        except Exception as e:
            self.logger.error(f"çˆ¬å–å¸–å­å¤±è´¥: {e}")
            return 0

    async def crawl_comments(self, page, post_url, max_comments=5):
        """çˆ¬å–å•ä¸ªå¸–å­çš„è¯„è®º"""
        try:
            self.logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­è¯„è®º: {post_url}")

            await page.goto(post_url, wait_until='domcontentloaded', timeout=45000)
            await asyncio.sleep(3)

            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°è¯„è®º
            comment_selectors = [
                '.reply-item',
                '.comment-item',
                '.stock-comment',
                '[class*="comment"]',
                '[class*="reply"]'
            ]

            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªè¯„è®º")
                        comment_elements = elements
                        break
                except:
                    continue

            for i, element in enumerate(comment_elements[:max_comments]):
                try:
                    # æå–è¯„è®ºå†…å®¹
                    content = await element.inner_text()

                    comment_data = {
                        'author': f'è¯„è®ºç”¨æˆ·{i+1}',
                        'content': content.strip()[:500] if content else '',
                        'comment_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'floor_number': i+1,
                        'post_url': post_url,
                        'stock_code': self.stock_code,
                        'crawl_time': datetime.now().isoformat()
                    }

                    self.comments_data.append(comment_data)

                except Exception as e:
                    self.logger.error(f"å¤„ç†è¯„è®º {i+1} æ—¶å‡ºé”™: {e}")
                    continue

            return len(comment_elements[:max_comments])

        except Exception as e:
            self.logger.error(f"çˆ¬å–è¯„è®ºå¤±è´¥: {e}")
            return 0


async def main() -> None:
    """ä¸»å‡½æ•° - Apify Actor å…¥å£"""
    async with Actor:
        # è®¾ç½®æ—¥å¿—
        Actor.log.info("ğŸš€ ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™«å¯åŠ¨...")

        # è·å–è¾“å…¥å‚æ•°
        actor_input = await Actor.get_input() or {}
        stock_code = actor_input.get('stockCode', '002001')
        stock_name = actor_input.get('stockName', 'æ–°å’Œæˆ')
        max_posts = actor_input.get('maxPosts', 10)
        crawl_comments = actor_input.get('crawlComments', False)
        headless = actor_input.get('headless', True)

        Actor.log.info(f"è¾“å…¥å‚æ•°: è‚¡ç¥¨ä»£ç ={stock_code}, è‚¡ç¥¨åç§°={stock_name}, æœ€å¤§å¸–å­æ•°={max_posts}")
        Actor.log.info(f"çˆ¬å–è¯„è®º: {crawl_comments}, æ— å¤´æ¨¡å¼: {headless}")

        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = EastMoneyCrawler(
            stock_code=stock_code,
            stock_name=stock_name,
            max_posts=max_posts,
            headless=headless
        )

        # å¯åŠ¨ Playwright
        Actor.log.info("æ­£åœ¨å¯åŠ¨ Playwright æµè§ˆå™¨...")

        async with async_playwright() as playwright:
            browser = await playwright.chromium.launch(
                headless=headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                ]
            )

            context = await browser.new_context()
            page = await context.new_page()

            # è®¾ç½®è¶…æ—¶æ—¶é—´
            page.set_default_timeout(60000)
            page.set_default_navigation_timeout(60000)

            Actor.log.info("âœ… Playwright æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")

            # çˆ¬å–å¸–å­åˆ—è¡¨
            posts_count = await crawler.crawl_post_list(page)
            Actor.log.info(f"âœ… æˆåŠŸçˆ¬å– {posts_count} ä¸ªå¸–å­")

            # å°†å¸–å­æ•°æ®æ¨é€åˆ° Apify æ•°æ®é›†
            if crawler.posts_data:
                await Actor.push_data(crawler.posts_data)
                Actor.log.info(f"âœ… å·²å°† {len(crawler.posts_data)} ä¸ªå¸–å­æ¨é€åˆ°æ•°æ®é›†")

            # å¦‚æœéœ€è¦çˆ¬å–è¯„è®º
            if crawl_comments and crawler.posts_data:
                Actor.log.info("å¼€å§‹çˆ¬å–å¸–å­è¯„è®º...")

                for i, post in enumerate(crawler.posts_data[:5]):  # é™åˆ¶åªçˆ¬å–å‰5ä¸ªå¸–å­çš„è¯„è®º
                    if post.get('post_url'):
                        Actor.log.info(f"çˆ¬å–ç¬¬ {i+1}/{min(5, len(crawler.posts_data))} ä¸ªå¸–å­çš„è¯„è®º")
                        comments_count = await crawler.crawl_comments(page, post['post_url'])
                        Actor.log.info(f"æ‰¾åˆ° {comments_count} æ¡è¯„è®º")
                        await asyncio.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

                # å°†è¯„è®ºæ•°æ®æ¨é€åˆ° Apify æ•°æ®é›†
                if crawler.comments_data:
                    await Actor.push_data(crawler.comments_data)
                    Actor.log.info(f"âœ… å·²å°† {len(crawler.comments_data)} æ¡è¯„è®ºæ¨é€åˆ°æ•°æ®é›†")

            # å…³é—­æµè§ˆå™¨
            await browser.close()

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        Actor.log.info("=== çˆ¬å–å®Œæˆ ===")
        Actor.log.info(f"æ€»å¸–å­æ•°: {len(crawler.posts_data)}")
        Actor.log.info(f"æ€»è¯„è®ºæ•°: {len(crawler.comments_data)}")
        Actor.log.info("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    asyncio.run(main())
