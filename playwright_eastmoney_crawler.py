#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨åŸç”ŸPlaywrightçš„ä¸œæ–¹è´¢å¯Œçˆ¬è™«
é¿å¼€AgentQLå…¼å®¹æ€§é—®é¢˜
"""

import asyncio
import logging
import json
from datetime import datetime
import re

class PlaywrightEastMoneyCrawler:
    """ä½¿ç”¨åŸç”ŸPlaywrightçš„ä¸œæ–¹è´¢å¯Œçˆ¬è™«"""
    
    def __init__(self, stock_code="002001", stock_name="æ–°å’Œæˆ"):
        self.stock_code = stock_code
        self.stock_name = stock_name
        self.base_url = f"https://guba.eastmoney.com/list,{stock_code}.html"
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('playwright_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # å­˜å‚¨æ•°æ®
        self.posts_data = []
        self.comments_data = []
    
    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            from playwright.async_api import async_playwright
            
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–Playwrightæµè§ˆå™¨...")
            
            # å¯åŠ¨Playwright
            self.playwright = await async_playwright().start()
            
            # å¯åŠ¨æµè§ˆå™¨
            self.browser = await self.playwright.chromium.launch(
                headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨ä¾¿äºè°ƒè¯•
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                ]
            )
            
            # åˆ›å»ºé¡µé¢
            self.page = await self.browser.new_page()
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´
            self.page.set_default_timeout(60000)
            self.page.set_default_navigation_timeout(60000)
            
            self.logger.info("âœ… Playwrightæµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def safe_goto(self, url, max_retries=3):
        """å®‰å…¨çš„é¡µé¢å¯¼èˆª"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt+1}æ¬¡): {url}")
                
                await self.page.goto(url, wait_until='domcontentloaded', timeout=45000)
                await asyncio.sleep(3)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
                title = await self.page.title()
                if title and ("ä¸œæ–¹è´¢å¯Œ" in title or "è‚¡å§" in title):
                    self.logger.info(f"âœ… é¡µé¢åŠ è½½æˆåŠŸ: {title}")
                    return True
                else:
                    self.logger.warning(f"é¡µé¢æ ‡é¢˜å¼‚å¸¸: {title}")
                    
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt+1}æ¬¡è®¿é—®å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5)
                else:
                    self.logger.error("æ‰€æœ‰è®¿é—®å°è¯•éƒ½å¤±è´¥äº†")
                    return False
        
        return False
    
    async def crawl_posts(self):
        """çˆ¬å–å¸–å­åˆ—è¡¨"""
        try:
            self.logger.info(f"å¼€å§‹çˆ¬å–è‚¡ç¥¨ {self.stock_name}({self.stock_code}) çš„å¸–å­...")
            
            # è®¿é—®é¡µé¢
            if not await self.safe_goto(self.base_url):
                return 0
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await asyncio.sleep(5)
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°å¸–å­åˆ—è¡¨
            selectors = [
                'tr.listitem',
                'tr[class*="listitem"]',
                '.articleh',
                '.normal_post',
                'tbody tr'
            ]
            
            post_elements = []
            for selector in selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        post_elements = elements
                        break
                except Exception as e:
                    self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue
            
            if not post_elements:
                self.logger.warning("æœªæ‰¾åˆ°å¸–å­å…ƒç´ ï¼Œå°è¯•è·å–é¡µé¢å†…å®¹è¿›è¡Œåˆ†æ")
                
                # è·å–é¡µé¢HTMLè¿›è¡Œåˆ†æ
                content = await self.page.content()
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¸–å­ä¿¡æ¯
                title_pattern = r'<a[^>]*title="([^"]*)"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
                matches = re.findall(title_pattern, content)
                
                self.logger.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(matches)} ä¸ªæ ‡é¢˜é“¾æ¥")
                
                for i, (title, href, text) in enumerate(matches[:10]):
                    if self.stock_code in href or "guba.eastmoney.com" in href:
                        post_url = href if href.startswith('http') else f"https://guba.eastmoney.com{href}"
                        
                        post_data = {
                            'title': title or text,
                            'author': f'ç”¨æˆ·{i+1}',
                            'post_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'read_count': '0',
                            'reply_count': '0',
                            'post_url': post_url,
                            'stock_code': self.stock_code,
                            'stock_name': self.stock_name,
                            'crawl_time': datetime.now().isoformat()
                        }
                        
                        self.posts_data.append(post_data)
                        self.logger.info(f"å¸–å­ {len(self.posts_data)}: {title[:50]}...")
                
                return len(self.posts_data)
            
            # å¤„ç†æ‰¾åˆ°çš„å¸–å­å…ƒç´ 
            for i, element in enumerate(post_elements[:10]):
                try:
                    # å°è¯•æå–æ ‡é¢˜
                    title_element = await element.query_selector('a[title]')
                    if not title_element:
                        title_element = await element.query_selector('a')
                    
                    if title_element:
                        title = await title_element.get_attribute('title')
                        if not title:
                            title = await title_element.inner_text()
                        
                        href = await title_element.get_attribute('href')
                        post_url = href if href and href.startswith('http') else f"https://guba.eastmoney.com{href}" if href else ""
                    else:
                        title = f"å¸–å­{i+1}"
                        post_url = ""
                    
                    # å°è¯•æå–å…¶ä»–ä¿¡æ¯
                    author = "æœªçŸ¥ç”¨æˆ·"
                    try:
                        author_element = await element.query_selector('.author, .username, [class*="author"]')
                        if author_element:
                            author = await author_element.inner_text()
                    except:
                        pass
                    
                    post_data = {
                        'title': title.strip() if title else f"å¸–å­{i+1}",
                        'author': author.strip() if author else f"ç”¨æˆ·{i+1}",
                        'post_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'read_count': '0',
                        'reply_count': '0',
                        'post_url': post_url,
                        'stock_code': self.stock_code,
                        'stock_name': self.stock_name,
                        'crawl_time': datetime.now().isoformat()
                    }
                    
                    self.posts_data.append(post_data)
                    self.logger.info(f"å¸–å­ {i+1}: {post_data['title'][:50]}...")
                    
                except Exception as e:
                    self.logger.error(f"å¤„ç†å¸–å­ {i+1} æ—¶å‡ºé”™: {e}")
                    continue
            
            return len(self.posts_data)
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–å¸–å­å¤±è´¥: {e}")
            return 0
    
    async def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜å¸–å­æ•°æ®
            posts_file = f"playwright_posts_{self.stock_code}_{timestamp}.json"
            with open(posts_file, 'w', encoding='utf-8') as f:
                json.dump(self.posts_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜è¯„è®ºæ•°æ®
            comments_file = f"playwright_comments_{self.stock_code}_{timestamp}.json"
            with open(comments_file, 'w', encoding='utf-8') as f:
                json.dump(self.comments_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… æ•°æ®å·²ä¿å­˜:")
            self.logger.info(f"   å¸–å­æ•°æ®: {posts_file}")
            self.logger.info(f"   è¯„è®ºæ•°æ®: {comments_file}")
            
            return posts_file, comments_file
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return None, None
    
    async def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            self.logger.info("=== å¼€å§‹Playwrightçˆ¬è™«ä»»åŠ¡ ===")
            
            # åˆå§‹åŒ–æµè§ˆå™¨
            if not await self.init_browser():
                return False
            
            # çˆ¬å–å¸–å­
            posts_count = await self.crawl_posts()
            
            if posts_count > 0:
                self.logger.info(f"æˆåŠŸçˆ¬å– {posts_count} ä¸ªå¸–å­")
                
                # ä¿å­˜æ•°æ®
                await self.save_data()
                
                # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                self.logger.info("=== çˆ¬å–å®Œæˆ ===")
                self.logger.info(f"æ€»å¸–å­æ•°: {len(self.posts_data)}")
                self.logger.info(f"æ€»è¯„è®ºæ•°: {len(self.comments_data)}")
                
                return True
            else:
                self.logger.error("æœªèƒ½çˆ¬å–åˆ°ä»»ä½•å¸–å­")
                return False
                
        except Exception as e:
            self.logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            return False
        
        finally:
            # æ¸…ç†èµ„æº
            try:
                if hasattr(self, 'browser'):
                    await self.browser.close()
                if hasattr(self, 'playwright'):
                    await self.playwright.stop()
                self.logger.info("æµè§ˆå™¨èµ„æºå·²æ¸…ç†")
            except Exception as e:
                self.logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    print("=== Playwrightä¸œæ–¹è´¢å¯Œçˆ¬è™« ===")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import playwright
        print("âœ… Playwrightä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘Playwrightä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install playwright")
        print("ç„¶åè¿è¡Œ: playwright install")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = PlaywrightEastMoneyCrawler()
    
    # è¿è¡Œçˆ¬è™«
    success = await crawler.run()
    
    if success:
        print("ğŸ‰ çˆ¬è™«ä»»åŠ¡å®Œæˆï¼")
    else:
        print("âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    asyncio.run(main())